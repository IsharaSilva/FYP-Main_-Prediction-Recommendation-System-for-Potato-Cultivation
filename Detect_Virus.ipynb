{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'F:/projevt/Training/'\n",
    "classes = {'Insect_Coloradopotatobeetle': 0, 'Insect_Fleabeetle': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file names and corresponding labels\n",
    "data = []\n",
    "labels = []\n",
    "for class_name, label in classes.items():\n",
    "    class_path = os.path.join(data_path, class_name)\n",
    "    file_names = os.listdir(class_path)\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(class_path, file_name)\n",
    "        data.append(file_path)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Split the data into training, validation, and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Background removal using thresholding\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresholded = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    image[thresholded == 0] = [0, 0, 0]  # Set background pixels to black\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Contrast improvement and histogram equalization\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    image = clahe.apply(gray)\n",
    "\n",
    "    # Gamma correction\n",
    "    gamma = 1.5\n",
    "    image = np.power(image / 255.0, gamma)\n",
    "    image = np.uint8(image * 255)\n",
    "\n",
    "    # Smoothing\n",
    "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "train_images = [preprocess_image(image_path) for image_path in train_data]\n",
    "val_images = [preprocess_image(image_path) for image_path in val_data]\n",
    "test_images = [preprocess_image(image_path) for image_path in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 256\n",
      "Validation set size: 64\n",
      "Testing set size: 80\n"
     ]
    }
   ],
   "source": [
    "# Print the size of each set\n",
    "print('Training set size:', len(train_data))\n",
    "print('Validation set size:', len(val_data))\n",
    "print('Testing set size:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images and labels to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "val_images = np.array(val_images)\n",
    "test_images = np.array(test_images)\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTOR function\n",
    "def feature_extractor(dataset):\n",
    "    x_train = dataset\n",
    "    image_dataset = pd.DataFrame()\n",
    "    for image in range(x_train.shape[0]):\n",
    "        df = pd.DataFrame()\n",
    "        img = x_train[image]\n",
    "\n",
    "        # FEATURE 1 - Pixel values\n",
    "        pixel_values = img.reshape(-1)\n",
    "        df['Pixel_Value'] = pixel_values\n",
    "\n",
    "        # FEATURE 2 - Gabor filter responses\n",
    "        num = 1\n",
    "        kernels = []\n",
    "        for theta in range(2):\n",
    "            theta = theta / 4. * np.pi\n",
    "            for sigma in (1, 3):\n",
    "                lamda = np.pi / 4\n",
    "                gamma = 0.5\n",
    "                gabor_label = 'Gabor' + str(num)\n",
    "                kernel = cv2.getGaborKernel((9, 9), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
    "                kernels.append(kernel)\n",
    "                fimg = cv2.filter2D(img, cv2.CV_8UC3, kernel)\n",
    "                filtered_img = fimg.reshape(-1)\n",
    "                df[gabor_label] = filtered_img\n",
    "                num += 1\n",
    "\n",
    "        # Append features from the current image to the dataset\n",
    "        image_dataset = pd.concat([df, image_dataset])\n",
    "\n",
    "    return image_dataset\n",
    "\n",
    "# Extract features from training images\n",
    "image_features = feature_extractor(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for SVM training\n",
    "n_samples = train_images.shape[0]\n",
    "image_features = np.array(image_features)\n",
    "image_features = np.reshape(image_features, (n_samples, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "SVM_model = SVC(kernel='rbf', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on training data\n",
    "SVM_model.fit(image_features, train_labels)\n",
    "\n",
    "# Save the trained model\n",
    "with open('F:/projevt/', 'wb') as file:\n",
    "    pickle.dump(SVM_model, file)\n",
    "\n",
    "# Extract features from test data and reshape\n",
    "test_features = feature_extractor(test_images)\n",
    "test_features = np.array(test_features)\n",
    "test_features = np.reshape(test_features, (test_images.shape[0], -1))\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = SVM_model.predict(test_features)\n",
    "\n",
    "# Convert labels back to original class names\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "test_predictions = le.inverse_transform(test_predictions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
